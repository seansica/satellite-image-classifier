{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score,confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class MultiClassLogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MultiClassLogisticRegression, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load training and testing data\n",
    "train_csv_path = \"train_labels_cleaned.csv\"\n",
    "test_csv_path = \"test_labels_cleaned.csv\"\n",
    "valid_csv_path = \"validate_labels_cleaned.csv\"\n",
    "\n",
    "train_data = pd.read_csv(train_csv_path)\n",
    "valid_data = pd.read_csv(valid_csv_path)\n",
    "test_data = pd.read_csv(test_csv_path)\n",
    "\n",
    "# Ensure both CSVs have 'filepath' and 'label' columns\n",
    "train_file_paths = train_data['image_path'].values\n",
    "train_labels = train_data['class'].values\n",
    "\n",
    "test_file_paths = test_data['image_path'].values\n",
    "test_labels = test_data['class'].values\n",
    "\n",
    "valid_file_paths = valid_data['image_path'].values\n",
    "valid_labels = valid_data['class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = np.unique(train_labels)\n",
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = 178  # Number of features\n",
    "output_dim = 11  # Number of classes\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "\n",
    "X_train = np.load(\"./resnet101_train_data_pca.npy\")\n",
    "y_train = label_encoder.fit_transform(train_labels)\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "# Prepare DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = np.load(\"./resnet101_val_data_pca.npy\")\n",
    "y_val = label_encoder.fit_transform(valid_labels)\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.load(\"./resnet101_test_data_pca.npy\")\n",
    "y_test = label_encoder.fit_transform(test_labels)\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Prepare DataLoader\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'labels' is a list of label names\n",
    "# Example: labels = ['Class 0', 'Class 1', ..., 'Class n']\n",
    "\n",
    "# Binarize the true labels (for ROC)\n",
    "\n",
    "def save_roc_curve(y_true, y_prob, class_labels, path):\n",
    "\n",
    "    num_classes = 11\n",
    "    y_true_bin = label_binarize(y_true, classes=np.arange(num_classes))  # num_classes should be set to the number of classes in your dataset\n",
    "\n",
    "    # Compute ROC curve and ROC AUC for each class\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    roc_auc = {}\n",
    "    for i in range(num_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], np.array(y_prob)[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Plot ROC curves\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i in range(num_classes):\n",
    "        plt.plot(fpr[i], tpr[i], label=f'{class_labels[i]} (AUC = {roc_auc[i]:.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for random classifier\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve for Multiclass Classification')\n",
    "    plt.legend(loc='lower right')\n",
    "    # plt.show()\n",
    "    plt.savefig(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_confusion_matrix(cf,\n",
    "                          group_names=None,\n",
    "                          categories='auto',\n",
    "                          count=True,\n",
    "                          percent=True,\n",
    "                          cbar=True,\n",
    "                          xyticks=True,\n",
    "                          xyplotlabels=True,\n",
    "                          sum_stats=True,\n",
    "                          figsize=None,\n",
    "                          cmap='Blues',\n",
    "                          title=None,\n",
    "                          path=None):\n",
    "    '''\n",
    "    This function will make a pretty plot of an sklearn Confusion Matrix cm using a Seaborn heatmap visualization.\n",
    "    Arguments\n",
    "    ---------\n",
    "    cf:            confusion matrix to be passed in\n",
    "    group_names:   List of strings that represent the labels row by row to be shown in each square.\n",
    "    categories:    List of strings containing the categories to be displayed on the x,y axis. Default is 'auto'\n",
    "    count:         If True, show the raw number in the confusion matrix. Default is True.\n",
    "    Percent:     If True, show the proportions for each category. Default is True.\n",
    "    cbar:          If True, show the color bar. The cbar values are based off the values in the confusion matrix.\n",
    "                   Default is True.\n",
    "    xyticks:       If True, show x and y ticks. Default is True.\n",
    "    xyplotlabels:  If True, show 'True Label' and 'Predicted Label' on the figure. Default is True.\n",
    "    sum_stats:     If True, display summary statistics below the figure. Default is True.\n",
    "    figsize:       Tuple representing the figure size. Default will be the matplotlib rcParams value.\n",
    "    cmap:          Colormap of the values displayed from matplotlib.pyplot.cm. Default is 'Blues'\n",
    "                   See http://matplotlib.org/examples/color/colormaps_reference.html            \n",
    "    title:         Title for the heatmap. Default is None.\n",
    "    '''\n",
    "\n",
    "\n",
    "    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n",
    "    blanks = ['' for i in range(cf.size)]\n",
    "\n",
    "    if group_names and len(group_names)==cf.size:\n",
    "        group_labels = [\"{}\\n\".format(value) for value in group_names]\n",
    "    else:\n",
    "        group_labels = blanks\n",
    "\n",
    "    if count:\n",
    "        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n",
    "    else:\n",
    "        group_counts = blanks\n",
    "\n",
    "    if percent:\n",
    "        # The original percent calculation is here, but its no good\n",
    "        # group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()/np.sum(cf)]\n",
    "        # Fixed below\n",
    "        cm_norm = cf.astype('float') / cf.sum(axis=1)[:, np.newaxis]\n",
    "        group_percentages = cm_norm.flatten()\n",
    "        group_percentages = np.around(group_percentages, 2)\n",
    "    else:\n",
    "        group_percentages = blanks\n",
    "\n",
    "    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]\n",
    "    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n",
    "\n",
    "\n",
    "    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n",
    "    if sum_stats:\n",
    "        #Accuracy is sum of diagonal divided by total observations\n",
    "        accuracy  = np.trace(cf) / float(np.sum(cf))\n",
    "\n",
    "        #if it is a binary confusion matrix, show some more stats\n",
    "        if len(cf)==2:\n",
    "            #Metrics for Binary Confusion Matrices\n",
    "            precision = cf[1,1] / sum(cf[:,1])\n",
    "            recall    = cf[1,1] / sum(cf[1,:])\n",
    "            f1_score  = 2*precision*recall / (precision + recall)\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(\n",
    "                accuracy,precision,recall,f1_score)\n",
    "        else:\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n",
    "    else:\n",
    "        stats_text = \"\"\n",
    "\n",
    "\n",
    "    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n",
    "    if figsize==None:\n",
    "        #Get default figure size if not set\n",
    "        figsize = plt.rcParams.get('figure.figsize')\n",
    "\n",
    "    if xyticks==False:\n",
    "        #Do not show categories if xyticks is False\n",
    "        categories=False\n",
    "\n",
    "    # Fix to color by percentages\n",
    "    if percent:\n",
    "        cf = cm_norm\n",
    "        \n",
    "    # MAKE THE HEATMAP VISUALIZATION\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cf,annot=box_labels,fmt=\"\",cmap=cmap,cbar=cbar,xticklabels=categories,yticklabels=categories)\n",
    "\n",
    "    if xyplotlabels:\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label' + stats_text)\n",
    "    else:\n",
    "        plt.xlabel(stats_text)\n",
    "    \n",
    "    if title:\n",
    "        plt.title(title)\n",
    "\n",
    "    plt.savefig(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with learning_rate=0.1, batch_size=64, weight_decay=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/125 [00:01<04:04,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Best model saved with validation loss = 1.3300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/125 [00:06<04:06,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Best model saved with validation loss = 1.2891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [04:04<00:00,  1.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for the best model saved to ./lgr_pca/lr_0.1_bs_64_wd_0\n",
      "Testing with learning_rate=0.1, batch_size=64, weight_decay=0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/125 [00:01<04:00,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Best model saved with validation loss = 1.3320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/125 [00:05<04:02,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Best model saved with validation loss = 1.3317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 4/125 [00:08<04:08,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Best model saved with validation loss = 1.3139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 5/125 [00:10<04:07,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Best model saved with validation loss = 1.3095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [04:11<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for the best model saved to ./lgr_pca/lr_0.1_bs_64_wd_0.0001\n",
      "Testing with learning_rate=0.1, batch_size=64, weight_decay=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/125 [00:02<04:10,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Best model saved with validation loss = 1.3634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/125 [00:03<04:03,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Best model saved with validation loss = 1.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/125 [00:05<04:01,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Best model saved with validation loss = 1.2804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 38/125 [01:16<02:51,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: Best model saved with validation loss = 1.2723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 70/125 [02:20<01:48,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70: Best model saved with validation loss = 1.2709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 88/125 [02:56<01:16,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88: Best model saved with validation loss = 1.2686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 99/125 [03:18<00:51,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: Best model saved with validation loss = 1.2636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [04:10<00:00,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for the best model saved to ./lgr_pca/lr_0.1_bs_64_wd_0.001\n",
      "CPU times: user 12min 32s, sys: 24.5 s, total: 12min 56s\n",
      "Wall time: 12min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import torch\n",
    "import itertools\n",
    "\n",
    "\n",
    "def save_confusion_matrix(y_true, y_pred, save_path):\n",
    "    cfm = confusion_matrix(y_true, y_pred)\n",
    "    make_confusion_matrix(cfm, percent = True, count= True, categories=class_labels, figsize = (10,10), path=save_path)\n",
    "\n",
    "# Define hyperparameter search space\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "learning_rates = [0.1]\n",
    "batch_sizes = [64]\n",
    "weight_decays = [0, 1e-4, 1e-3]\n",
    "epochs = 125\n",
    "\n",
    "# Create a list of all combinations of hyperparameters\n",
    "hyperparameter_combinations = list(itertools.product(learning_rates, batch_sizes, weight_decays))\n",
    "\n",
    "# Define base folder to save results\n",
    "results_base_folder = \"./lgr_pca/\"\n",
    "os.makedirs(results_base_folder, exist_ok=True)\n",
    "results_dict = {}\n",
    "\n",
    "# Training loop\n",
    "for lr, batch_size, weight_decay in hyperparameter_combinations:\n",
    "    # Create a subfolder for this hyperparameter combination\n",
    "    combination_folder = os.path.join(results_base_folder, f\"lr_{lr}_bs_{batch_size}_wd_{weight_decay}\")\n",
    "    os.makedirs(combination_folder, exist_ok=True)\n",
    "    \n",
    "    print(f\"Testing with learning_rate={lr}, batch_size={batch_size}, weight_decay={weight_decay}\")\n",
    "    \n",
    "    # Adjust DataLoader with the current batch_size\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)  # Add test loader\n",
    "    \n",
    "    # Instantiate the model, loss function, and optimizer\n",
    "    model = MultiClassLogisticRegression(input_dim, output_dim).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    best_val_loss = float('inf')  # Initialize to a large number\n",
    "    best_model_path = os.path.join(combination_folder, 'best_model.pth')  # Path to save the best model\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "    \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        y_val_true, y_val_pred = [], []\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                y_val_true.extend(labels.cpu().numpy())\n",
    "                y_val_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        # Save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_model_path)  # Save the best model\n",
    "            print(f\"Epoch {epoch+1}: Best model saved with validation loss = {val_loss:.4f}\")\n",
    "\n",
    "    # Reload the best model\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    model.eval()\n",
    "\n",
    "    # Evaluation metrics on train, val, and test sets\n",
    "    def evaluate(loader):\n",
    "        y_true, y_pred = [], []\n",
    "        total_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                y_true.extend(labels.cpu().numpy())\n",
    "                y_pred.extend(predicted.cpu().numpy())\n",
    "        total_loss /= len(loader)\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred, average='weighted')\n",
    "        recall = recall_score(y_true, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "        return total_loss, accuracy, precision, recall, f1\n",
    "\n",
    "    train_loss, train_accuracy, train_precision, train_recall, train_f1 = evaluate(train_loader)\n",
    "    val_loss, val_accuracy, val_precision, val_recall, val_f1 = evaluate(val_loader)\n",
    "    test_loss, test_accuracy, test_precision, test_recall, test_f1 = evaluate(test_loader)\n",
    "\n",
    "    # Save results for the best model\n",
    "    results = {\n",
    "        \"learning_rate\": lr,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"train_metrics\": {\n",
    "            \"loss\": train_loss,\n",
    "            \"accuracy\": train_accuracy,\n",
    "            \"precision\": train_precision,\n",
    "            \"recall\": train_recall,\n",
    "            \"f1\": train_f1,\n",
    "        },\n",
    "        \"val_metrics\": {\n",
    "            \"loss\": val_loss,\n",
    "            \"accuracy\": val_accuracy,\n",
    "            \"precision\": val_precision,\n",
    "            \"recall\": val_recall,\n",
    "            \"f1\": val_f1,\n",
    "        },\n",
    "        \"test_metrics\": {\n",
    "            \"loss\": test_loss,\n",
    "            \"accuracy\": test_accuracy,\n",
    "            \"precision\": test_precision,\n",
    "            \"recall\": test_recall,\n",
    "            \"f1\": test_f1,\n",
    "        }\n",
    "    }\n",
    "    results_dict[combination_folder] = results\n",
    "    # Save metrics to a JSON file\n",
    "    metrics_path = os.path.join(combination_folder, \"metrics.json\")\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "    print(f\"Results for the best model saved to {combination_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "res",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
